{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zNPLcZJzM1lH"
   },
   "source": [
    "# Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "LUCvPRWBM1lJ",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.480557Z",
     "end_time": "2023-08-11T13:24:49.710652Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random as rd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xKyGuiH9M1lK"
   },
   "source": [
    "# Initiate lists of data to load into training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GJkEIz8SM1lK"
   },
   "source": [
    "## Product Types list and query funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CwvRbfwqM1lK",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.503561Z",
     "end_time": "2023-08-11T13:24:49.741213Z"
    }
   },
   "outputs": [],
   "source": [
    "def product_list_creation(file_name: str) -> list:\n",
    "    products_df = pd.read_excel(file_name, sheet_name='product_type')\n",
    "\n",
    "    unique_family = products_df['Family'].dropna().unique().tolist()\n",
    "    product_types = products_df['ProductType'].dropna().tolist()\n",
    "    keywords = [i.split(', ') for i in products_df['Keywords'].dropna().tolist()]\n",
    "    keyword_list = list(set([w for word in keywords for w in word]))\n",
    "\n",
    "    product_types.extend(keyword_list)\n",
    "    product_types.extend(unique_family)\n",
    "    product_types = list(set(product_types))\n",
    "    return product_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LP70Nu_AM1lK",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.508995Z",
     "end_time": "2023-08-11T13:24:49.950939Z"
    }
   },
   "outputs": [],
   "source": [
    "def product_query(product_types):\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for word in product_types:\n",
    "        labels_product = ['B-Product'] + ['I-Product'] * (len(word.split(' ')) - 1)\n",
    "        queries.append(word.split(' '))\n",
    "        labels.append(labels_product)\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nzEQgGfWM1lK"
   },
   "source": [
    "## Material Types list and query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bE3IOx2QM1lL",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.517518Z",
     "end_time": "2023-08-11T13:24:49.956514Z"
    }
   },
   "outputs": [],
   "source": [
    "def material_list_creation(file_name: str) -> list:\n",
    "    materials_df = pd.read_excel(file_name, sheet_name='material_type')\n",
    "\n",
    "    material_types = materials_df['material_type'].dropna().tolist()\n",
    "    mat_fam = materials_df['family'].dropna().tolist()\n",
    "    mat_k_word = list(set([w for words in [i.split(', ') for i in materials_df['keywords'].dropna().tolist()] for w in words]))\n",
    "\n",
    "    material_types.extend(mat_fam)\n",
    "    material_types.extend(mat_k_word)\n",
    "    material_types = list(set(material_types))\n",
    "    return material_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QkjjMQkpM1lL",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.524068Z",
     "end_time": "2023-08-11T13:24:49.998411Z"
    }
   },
   "outputs": [],
   "source": [
    "def material_query(material_types):\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for word in material_types:\n",
    "        word = word.lower()\n",
    "        labels_material = ['B-Material'] + ['I-Material'] * (len(word.split(' ')) - 1)\n",
    "        queries.append(word.split(' '))\n",
    "        labels.append(labels_material)\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nuMNqP5GM1lL"
   },
   "source": [
    "## Building Applications list and query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NMo0yzW_M1lL",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.533775Z",
     "end_time": "2023-08-11T13:24:50.001784Z"
    }
   },
   "outputs": [],
   "source": [
    "def building_app_list_creation(file_name: str) -> list:\n",
    "    building_app_df = pd.read_excel(file_name, sheet_name='building_applications')\n",
    "\n",
    "    building_app_list = building_app_df['building_applications'].dropna().tolist()\n",
    "    keywords = list(set([w for words in [i.split(', ') for i in building_app_df['keywords'].dropna().tolist()] for w in words]))\n",
    "    building_app_list.extend(keywords)\n",
    "    return building_app_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9Ce4cVrCM1lL",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.538823Z",
     "end_time": "2023-08-11T13:24:50.050608Z"
    }
   },
   "outputs": [],
   "source": [
    "def building_app_query(building_app_list):\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for word in building_app_list:\n",
    "        labels_building_application = ['B-Application'] + ['I-Application'] * (len(word.split(' ')) - 1)\n",
    "        queries.append(word.split(' '))\n",
    "        labels.append(labels_building_application)\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tqVrI1LXM1lL"
   },
   "source": [
    "## Country list and query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "mSagFIB1M1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.549549Z",
     "end_time": "2023-08-11T13:24:50.050608Z"
    }
   },
   "outputs": [],
   "source": [
    "def country_list_creation(file_name: str) -> list:\n",
    "    country_df = pd.read_excel(file_name, sheet_name='countries')\n",
    "    country_list = country_df['country'].dropna().tolist()\n",
    "    return country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "h3gxZyQyM1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.553234Z",
     "end_time": "2023-08-11T13:24:50.099608Z"
    }
   },
   "outputs": [],
   "source": [
    "def country_query(country_list):\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for word in country_list:\n",
    "        labels_country = ['B-Country'] + ['I-Country'] * (len(word.split(' ')) - 1)\n",
    "        queries.append(word.split(' '))\n",
    "        labels.append(labels_country)\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ODvY-IcrM1lM"
   },
   "source": [
    "## Recycled Content list and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "eY6ssBlPM1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.567056Z",
     "end_time": "2023-08-11T13:24:50.109361Z"
    }
   },
   "outputs": [],
   "source": [
    "def recycled_list_creation(file_name: str) -> list:\n",
    "    recycled_content_df = pd.read_excel(file_name, sheet_name='recycled_content')\n",
    "    recycled_list = recycled_content_df['recycled_content'].dropna().str.lower().tolist()\n",
    "    recycle_keywords = list(set([w for words in [i.split(', ') for i in recycled_content_df['keywords'].dropna().tolist()] for w in words]))\n",
    "    recycled_list.extend(recycle_keywords)\n",
    "\n",
    "    final_list = []\n",
    "    for item in recycled_list:\n",
    "        if 'x%' in item:\n",
    "            for _ in range(21):\n",
    "                final_list.append(item.replace('x%', str(rd.randint(0, 100)) + '%'))\n",
    "        elif '.x' in item:\n",
    "            for _ in range(21):\n",
    "                final_list.append(item.replace('.x', '.' + str(rd.randint(0, 100))))\n",
    "        else:\n",
    "            final_list.append(item)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IGox0hFjM1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.572748Z",
     "end_time": "2023-08-11T13:24:50.112902Z"
    }
   },
   "outputs": [],
   "source": [
    "def recycle_query(recycled_list):\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for word in recycled_list:\n",
    "        labels_country = ['B-Recycle'] + ['I-Recycle'] * (len(word.split(' ')) - 1)\n",
    "        queries.append(word.split(' '))\n",
    "        labels.append(labels_country)\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "IfKF654xM1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.578814Z",
     "end_time": "2023-08-11T13:24:50.112902Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXCEL FILE WITH SHEETS OF UNIQUE PRODUCT/MATERIAL/BUILDINGAPPLICATION TYPE REDACTED\n",
    "PROPRIETARY KNOWLEDGE OF 2050 MATERIALS\n",
    "CONTACT info@2050-materials.com FOR MORE INFORMATION\n",
    "'''\n",
    "def list_initialization(file_name: str ='YOUR_EXCEL_FILE.xlsx'):\n",
    "    product_types = product_list_creation(file_name)\n",
    "    material_types = material_list_creation(file_name)\n",
    "    building_app_list = building_app_list_creation(file_name)\n",
    "    country_list = country_list_creation(file_name)\n",
    "    recycled_list = recycled_list_creation(file_name)\n",
    "\n",
    "    return product_types, material_types, building_app_list, country_list, recycled_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "V264LKTkM1lM"
   },
   "source": [
    "## Creates query with proper label for every word in product types, material types, building applications, countries, and recycled content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HlzFFJ4QM1lM",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.584530Z",
     "end_time": "2023-08-11T13:24:50.153809Z"
    }
   },
   "outputs": [],
   "source": [
    "def all_queries(product_types, material_types, building_app_list, country_list, recycled_list):\n",
    "    queries = []\n",
    "    labels = []\n",
    "\n",
    "    p_queries, p_labels = product_query(product_types)\n",
    "    queries.extend(p_queries)\n",
    "    labels.extend(p_labels)\n",
    "\n",
    "    m_queries, m_labels = material_query(material_types)\n",
    "    queries.extend(m_queries)\n",
    "    labels.extend(m_labels)\n",
    "\n",
    "    b_queries, b_labels = building_app_query(building_app_list)\n",
    "    queries.extend(b_queries)\n",
    "    labels.extend(b_labels)\n",
    "\n",
    "    c_queries, c_labels = country_query(country_list)\n",
    "    queries.extend(c_queries)\n",
    "    labels.extend(c_labels)\n",
    "\n",
    "    r_queries, r_labels = recycle_query(recycled_list)\n",
    "    queries.extend(r_queries)\n",
    "    labels.extend(r_labels)\n",
    "\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Y_-882I9M1lM"
   },
   "source": [
    "## Creation of template sentences, all words in database added with \"all_words='y'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KeqHRiPyM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.597969Z",
     "end_time": "2023-08-11T13:24:50.156239Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_sentences(num, product_types, material_types, building_app_list, country_list, recycled_list, all_words='n'):\n",
    "    # template sentences that selects information at random to be selected for training data\n",
    "    templates = [\n",
    "                 \"{product_type} products manufactured {country} for {building_application}\",\n",
    "                 \"{material_type} manufactured in {country} for {building_application} with {recycled_content} recycled content\",\n",
    "                 \"{product_type} manufactured {country} with {recycled_content} recycled content\",\n",
    "                 \"{product_type} made in {country} for {building_application} with {recycled_content} recycle content\",\n",
    "                 \"Find {product_type} from {country} used in {building_application}\",\n",
    "                 \"Do you have any {product_type} used in {building_application}?\",\n",
    "                 \"I'm looking for {material_type} products from {country} for {building_application}.\",\n",
    "                 \"{product_type} from {country} suitable for {building_application}?\",\n",
    "                 \"Search for {material_type} materials from {country} used in {building_application} applications\",\n",
    "                 \"Can I find {product_type} suitable for {building_application} applications?\",\n",
    "                 \"What types of {material_type} from {country} are used in {building_application}?\",\n",
    "                 \"Show me {product_type} suitable for {building_application}\",\n",
    "                 \"List all {material_type} from {country} used in {building_application}.\",\n",
    "                 \"Are there any {product_type} from {country} for {building_application}?\",\n",
    "                 \"Find {product_type} with {recycled_content} recycled content.\",\n",
    "                 \"Find {material_type} with {recycled_content} recycled content.\",\n",
    "                 \"List all {material_type} from {country} with recycled content {recycled_content} \"\n",
    "                 ]\n",
    "\n",
    "    queries = []\n",
    "    labels = []\n",
    "\n",
    "    for null in range(num):\n",
    "        temp = rd.choice(templates)\n",
    "        material_type = rd.choice(material_types)\n",
    "        product_type = rd.choice(product_types)\n",
    "        building_application = rd.choice(building_app_list)\n",
    "        country = rd.choice(country_list)\n",
    "        recycled_content = rd.choice(recycled_list)\n",
    "        sentence = temp.format(\n",
    "                                material_type=material_type,\n",
    "                                product_type=product_type,\n",
    "                                building_application=building_application,\n",
    "                                country=country,\n",
    "                                recycled_content=recycled_content\n",
    "                                )\n",
    "\n",
    "        queries.append(sentence.lower())\n",
    "        # Generate labels\n",
    "        labels_product = ['B-Product'] + ['I-Product'] * (len(product_type.split(' ')) - 1)\n",
    "        labels_material = ['B-Material'] + ['I-Material'] * (len(material_type.split(' ')) - 1)\n",
    "        labels_country = ['B-Country'] + ['I-Country'] * (len(country.split(' ')) - 1)\n",
    "        labels_building_application = ['B-Application'] + ['I-Application'] * (len(building_application.split(' ')) - 1)\n",
    "        labels_recycled_content = ['B-Recycle'] + ['I-Recycle'] * (len(str(recycled_content).split(' ')) - 1)\n",
    "\n",
    "        # Identify the placeholders in the template and replace them with the corresponding labels\n",
    "        template_labels = temp.split()\n",
    "        new_template_labels = []\n",
    "        for word in template_labels:\n",
    "            if 'product_type' in word:\n",
    "                new_template_labels.extend(labels_product)\n",
    "            elif 'material_type' in word:\n",
    "                new_template_labels.extend(labels_material)\n",
    "            elif 'country' in word:\n",
    "                new_template_labels.extend(labels_country)\n",
    "            elif 'building_application' in word:\n",
    "                new_template_labels.extend(labels_building_application)\n",
    "            elif 'recycled_content' in word:\n",
    "                new_template_labels.extend(labels_recycled_content)\n",
    "            else:\n",
    "                new_template_labels.append('O')\n",
    "\n",
    "        template_labels = new_template_labels\n",
    "        labels.append(template_labels)\n",
    "\n",
    "    if all_words == 'y':\n",
    "        solo_queries, solo_labels = all_queries(product_types, material_types, building_app_list, country_list, recycled_list)\n",
    "        queries.extend(solo_queries)\n",
    "        labels.extend(solo_labels)\n",
    "        print(f'{len(solo_queries)} words from database and {num} random sentences added to queries and labels data.')\n",
    "    else:\n",
    "        print(f'{num} random sentences added to queries and labels data.')\n",
    "    return queries, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Czf1vejXM1lN"
   },
   "source": [
    "## CustomDataset class to process words for tokeninzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "5Bcm_rHHM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.604597Z",
     "end_time": "2023-08-11T13:24:50.162238Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len, pad_token_label_id):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_token_label_id = pad_token_label_id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = str(self.sentences[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        label = ['O'] + self.labels[index] + ['O']\n",
    "        label = [Token_Classification_Labels.get(l) for l in label]\n",
    "        padding_length = self.max_len - len(label)\n",
    "        label.extend([self.pad_token_label_id]*padding_length)\n",
    "        label = label[:self.max_len]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "X72umWxAM1lN"
   },
   "source": [
    "## Loads queries and labels into train dataset to prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Bj-rUZmxM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.621726Z",
     "end_time": "2023-08-11T13:24:50.205678Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_data_info(queries, labels, tokenizer):\n",
    "    # First, split into train+val and test sets\n",
    "    sentences_temp, sentences_test, labels_temp, labels_test = train_test_split(queries, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Then split train+val into separate train and val sets\n",
    "    sentences_train, sentences_val, labels_train, labels_val = train_test_split(sentences_temp, labels_temp, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Padding label\n",
    "    PAD_TOKEN_LABEL_ID = -100\n",
    "\n",
    "    # Now, prepare the training, validation, and test data\n",
    "    train_data = CustomDataset(tokenizer, sentences_train, labels_train, max_len=50, pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "    valid_data = CustomDataset(tokenizer, sentences_val, labels_val, max_len=50, pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "    test_data = CustomDataset(tokenizer, sentences_test, labels_test, max_len=50, pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "\n",
    "    # Initialize the data loaders\n",
    "    train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)  # we don't need to shuffle the test set\n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UBvT4DmaM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.629621Z",
     "end_time": "2023-08-11T13:24:50.245993Z"
    }
   },
   "outputs": [],
   "source": [
    "def Initialize_tokenizer():\n",
    "    from transformers import DistilBertTokenizerFast\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Y9Uc1rrmM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.632231Z",
     "end_time": "2023-08-11T13:24:50.247000Z"
    }
   },
   "outputs": [],
   "source": [
    "def Token_Classification_Initialization():\n",
    "    from transformers import DistilBertForTokenClassification\n",
    "    Token_Classification_Labels = {\n",
    "                                  'O': 0,\n",
    "                                  'B-Product': 1,\n",
    "                                  'I-Product': 2,\n",
    "                                  'B-Material': 3,\n",
    "                                  'I-Material': 4,\n",
    "                                  'B-Country': 5,\n",
    "                                  'I-Country': 6,\n",
    "                                  'B-Application': 7,\n",
    "                                  'I-Application': 8,\n",
    "                                  'B-Recycle': 9,\n",
    "                                  'I-Recycle': 10\n",
    "                                  }\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    Token_Classification_model = DistilBertForTokenClassification.from_pretrained(\n",
    "                                    \"distilbert-base-uncased\",\n",
    "                                    num_labels=len(Token_Classification_Labels)).to(device)\n",
    "    return Token_Classification_Labels, Token_Classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "LS8JMSLTM1lN",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.649698Z",
     "end_time": "2023-08-11T13:24:50.249334Z"
    }
   },
   "outputs": [],
   "source": [
    "def Token_Classification_Model_Training(\n",
    "                                        Token_Classification_Labels: dict,\n",
    "                                        Token_Classification_model,\n",
    "                                        train_dataloader,\n",
    "                                        test_dataloader,\n",
    "                                        Model_load_file: None|str = None,\n",
    "                                        learning_rate: float = 1e-5,\n",
    "                                        epoch_loop : int = 5\n",
    "                                        ) -> list:\n",
    "    optimizer = torch.optim.AdamW(params=Token_Classification_model.parameters(), lr=learning_rate)\n",
    "    # Loads model if there is provided file name\n",
    "    if Model_load_file:\n",
    "        Token_Classification_model.load_state_dict(torch.load(Model_load_file))\n",
    "        print(f'{Model_load_file} successfully loaded.')\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Token_Classification_model.to(device)\n",
    "\n",
    "    Token_Classification_model.train()\n",
    "\n",
    "    # Training loop\n",
    "    training_loss = []\n",
    "    raw_results = []\n",
    "    print('Training starting.')\n",
    "    for null in tqdm(range(epoch_loop)):  # loop over the dataset multiple time\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader, 0):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get the inputs\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = Token_Classification_model(inputs, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # add the loss to the list\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        Token_Classification_model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for i, batch in enumerate(test_dataloader):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = Token_Classification_model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs[0].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the predictions\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            true_labels.extend(label_ids)\n",
    "\n",
    "        # Define mapping from index to labels\n",
    "        idx2label = {v: k for k, v in Token_Classification_Labels.items()}\n",
    "        idx2label[-100] = 'PAD'\n",
    "\n",
    "        # Flatten the output tensors\n",
    "        predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "        true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Map indices back to labels\n",
    "        pred_tags_flat = [idx2label[p] for p in predictions_flat]\n",
    "        true_tags_flat = [idx2label[t] for t in true_labels_flat]\n",
    "\n",
    "        # Filter out 'PAD' labels\n",
    "        pred_tags = [pred for pred, true in zip(pred_tags_flat, true_tags_flat) if true != 'PAD']\n",
    "        true_tags = [true for true in true_tags_flat if true != 'PAD']\n",
    "\n",
    "        # Binarize the labels\n",
    "        lb = LabelBinarizer()\n",
    "        y_true_combined = lb.fit_transform(true_tags)\n",
    "        y_pred_combined = lb.transform(pred_tags)\n",
    "\n",
    "        # appends classification report into list for each epoch\n",
    "        class_report = classification_report(y_true_combined, y_pred_combined, target_names=lb.classes_, digits=4, zero_division=1, output_dict=True)\n",
    "        raw_results.append(class_report)\n",
    "        Token_Classification_model.train()\n",
    "\n",
    "        # Save training loss\n",
    "        training_loss.append(total_loss/len(train_dataloader))\n",
    "\n",
    "    raw_results.append(training_loss)\n",
    "    print('Training Complete.')\n",
    "    if Model_load_file:\n",
    "        torch.save(Token_Classification_model.state_dict(), Model_load_file)\n",
    "        print(f'{Model_load_file} successfully saved.')\n",
    "    else:\n",
    "        Model_load_file = 'Token_Class_Model.pth'\n",
    "        torch.save(Token_Classification_model.state_dict(), Model_load_file)\n",
    "        print(f'{Model_load_file} successfully saved.')\n",
    "    return raw_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2t6raSmSM1lO"
   },
   "source": [
    "## Organizes results from training into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5QSo8aXUM1lO",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.660032Z",
     "end_time": "2023-08-11T13:24:50.283110Z"
    }
   },
   "outputs": [],
   "source": [
    "def results_dict_creation(raw_results: list) -> dict:\n",
    "    training_loss = raw_results.pop()\n",
    "    results_dict = {\n",
    "                'epoch': [],\n",
    "                'precision': [],\n",
    "                'recall': [],\n",
    "                'f1_score': [],\n",
    "                'training loss': []\n",
    "                }\n",
    "    for index, item in enumerate(raw_results):\n",
    "        results_dict['epoch'].append(index+1)\n",
    "        results_dict['precision'].append(item['weighted avg']['precision'])\n",
    "        results_dict['recall'].append(item['weighted avg']['recall'])\n",
    "        results_dict['f1_score'].append(item['weighted avg']['f1-score'])\n",
    "        results_dict['training loss'].append(training_loss[index])\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWU8D08XNWde"
   },
   "source": [
    "## Takes Epoch df column and column to compare to output graph of progression over each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Hp8cvF9XM1lO",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.669137Z",
     "end_time": "2023-08-11T13:24:50.344564Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_creation(epochs, y_value):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epoch_nums = epochs.tolist()\n",
    "    y_val_list = y_value.tolist()\n",
    "\n",
    "    x = epochs.values.reshape(-1,1)\n",
    "    y = y_value.values.reshape(-1,1)\n",
    "\n",
    "    linear_r = LinearRegression()\n",
    "    linear_r.fit(x, y)\n",
    "    y_pred = linear_r.predict(x)\n",
    "    plt.plot(epoch_nums, y_pred, color='red', linestyle='dashdot')\n",
    "\n",
    "    plt.plot(epoch_nums, y_val_list, color='blue')\n",
    "    plt.title(y_value.name)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['linear regression',y_value.name])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFekdtU0Nfb-"
   },
   "source": [
    "## Saves results of training into .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "70JLERoRM1lO",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.675987Z",
     "end_time": "2023-08-11T13:24:50.344564Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_results(df):\n",
    "    import datetime\n",
    "    import pytz\n",
    "    '''\n",
    "    SAVES ALL TRAINING RESULTS, PRECISION RECALL F1_SCORE TRAINING LOSS\n",
    "    INTO .csv FILE IN YEAR-MONTH-DAY_HOUR-MINUTE-SECOND.csv FILE NAME,\n",
    "    LINKED WITH YOUR TIMEZONE SO NO DUPLICATE FILES AND KNOWLEDGE OF WHEN TRAINING IS COMPLETE\n",
    "    '''\n",
    "    current_time = datetime.datetime.now(pytz.timezone('REPLACE_WITH_YOUR_TIMEZONE'))\n",
    "    save_location = 'FOLDER_LOCATION_OF_TRAINING_DATA' + current_time.strftime('%Y-%m-%d_%H-%M-%S') + '.csv'\n",
    "    df.to_csv(save_location, sep=',', index=False)\n",
    "    print(f'File {save_location} saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function to take sentance/query from user and process result, output in dictionary {str:list}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Jezy0eu-M1lO",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.681909Z",
     "end_time": "2023-08-11T13:24:50.344564Z"
    }
   },
   "outputs": [],
   "source": [
    "def Token_Classification_Results(List_of_Sentences: list,\n",
    "                                 Token_Classification_Labels: dict,\n",
    "                                 Token_Classification_model,\n",
    "                                 tokenizer,\n",
    "                                 Token_Class_File: str\n",
    "                                 ) -> dict:\n",
    "    # Puts model into eval mode\n",
    "    Token_Classification_model.eval()\n",
    "\n",
    "    # Loads the saved file of Pre-trained model\n",
    "    Token_Classification_model.load_state_dict(torch.load(Token_Class_File))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Inverse mapping from label indices to labels\n",
    "    inv_label_map = {v: k for k, v in Token_Classification_Labels.items()}\n",
    "\n",
    "    # Creating Final Dictionary for results\n",
    "    final_dict = {\n",
    "                  'Product Type': [],\n",
    "                  'Material Type': [],\n",
    "                  'Building applications': [],\n",
    "                  'Countries': [],\n",
    "                  'Recycled Content': []\n",
    "                  }\n",
    "\n",
    "    # Cycling through query\n",
    "    for sentence in List_of_Sentences:\n",
    "        sentence = sentence.lower()\n",
    "        # Encode the sentence\n",
    "        inputs = tokenizer.encode_plus(\n",
    "                                        sentence,\n",
    "                                        None,\n",
    "                                        add_special_tokens=True,\n",
    "                                        padding='longest',\n",
    "                                        return_token_type_ids=True\n",
    "                                        )\n",
    "\n",
    "        # Create torch tensors and move them to the device\n",
    "        input_ids = torch.tensor([inputs['input_ids']], dtype=torch.long).to(device)\n",
    "        attention_mask = torch.tensor([inputs['attention_mask']], dtype=torch.long).to(device)\n",
    "\n",
    "        # Run the sentence through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = Token_Classification_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the token-level class probabilities\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Compute the predicted labels\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Remove padding and special tokens\n",
    "        input_ids = input_ids[0].tolist()\n",
    "        predictions = predictions[0].tolist()\n",
    "\n",
    "        # real_input_ids = [id for id, pred in zip(input_ids, predictions) if id != 0 and id != 101 and id != 102]\n",
    "        real_predictions = [pred for id, pred in zip(input_ids, predictions) if id != 0 and id != 101 and id != 102]\n",
    "\n",
    "        # real_input_ids = []\n",
    "        # real_predictions = []\n",
    "        #\n",
    "        # for id, pred in zip(input_ids, predictions):\n",
    "        #     if id != 0 and id != 101 and id != 102: # Ignore [PAD], [CLS] and [SEP] tokens\n",
    "        #         real_input_ids.append(id)\n",
    "        #         real_predictions.append(pred)\n",
    "\n",
    "        # Map predicted label indices back to label strings\n",
    "        predicted_labels = [inv_label_map[label] for label in real_predictions]\n",
    "\n",
    "        # Combine tokens and their predicted labels into dict\n",
    "        results = dict(zip(sentence.split(' '), predicted_labels))\n",
    "\n",
    "        # Put results into final_dict, key is word and value is what it is labeled as\n",
    "        for word, label in results.items():\n",
    "            print(word, label)\n",
    "            if word in ['for', 'and', 'from']:\n",
    "                continue\n",
    "            if label != 'O':\n",
    "                # Adds word to proper dictionary if belongs to that type\n",
    "\n",
    "                # Checks if there is a B-word before the I-word so it will add to it\n",
    "\n",
    "                # It will make a new word if there is no B-word\n",
    "\n",
    "                if label == 'B-Product':\n",
    "                    final_dict['Product Type'].append(word)\n",
    "\n",
    "                elif label == 'I-Product':\n",
    "\n",
    "                  if final_dict['Product Type']:\n",
    "                    final_dict['Product Type'][-1] += ' ' + word\n",
    "                  else:\n",
    "                    final_dict['Product Type'].append(word)\n",
    "\n",
    "                elif label == 'B-Material':\n",
    "                    final_dict['Material Type'].append(word)\n",
    "\n",
    "                elif label == 'I-Material':\n",
    "\n",
    "                  if final_dict['Material Type']:\n",
    "                    final_dict['Material Type'][-1] += ' ' + word\n",
    "                  else:\n",
    "                     final_dict['Material Type'].append(word)\n",
    "\n",
    "                elif label == 'B-Country':\n",
    "                    final_dict['Countries'].append(word)\n",
    "\n",
    "                elif label == 'I-Country':\n",
    "\n",
    "                  if final_dict['Countries']:\n",
    "                    final_dict['Countries'][-1] += ' ' + word\n",
    "                  else:\n",
    "                    final_dict['Countries'].append(word)\n",
    "\n",
    "                elif label == 'B-Application':\n",
    "                    final_dict['Building applications'].append(word)\n",
    "\n",
    "                elif label == 'I-Application':\n",
    "\n",
    "                  if final_dict['Building applications']:\n",
    "                    final_dict['Building applications'][-1] += ' ' + word\n",
    "                  else:\n",
    "                    final_dict['Building applications'].append(word)\n",
    "\n",
    "                elif label == 'B-Recycle':\n",
    "                    final_dict['Recycled Content'].append(word)\n",
    "\n",
    "                elif label == 'I-Recycle':\n",
    "\n",
    "                    if final_dict['Recycled Content']:\n",
    "                        final_dict['Recycled Content'][-1] += ' ' + word\n",
    "                    else:\n",
    "                        final_dict['Recycled Content'].append(word)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM3ij3QyPsIM"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPwCmB_iPt-Z"
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "FZgpV3KzM1lO",
    "outputId": "5c7aece4-4a7e-4859-c096-cbaf8d879d08",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:49.710652Z",
     "end_time": "2023-08-11T13:24:50.747361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "Token_Classification_Labels, Token_Classification_model = Token_Classification_Initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "4bZ7_wjeM1lP",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:50.487126Z",
     "end_time": "2023-08-11T13:24:50.748831Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXCEL FILE WITH SHEETS OF UNIQUE PRODUCT/MATERIAL/BUILDINGAPPLICATION TYPE REDACTED\n",
    "PROPRIETARY KNOWLEDGE OF 2050 MATERIALS\n",
    "CONTACT info@2050-materials.com FOR MORE INFORMATION\n",
    "'''\n",
    "excel_file = 'YOUR_EXCEL_FILE.xlsx'\n",
    "\n",
    "product_types, material_types, building_app_list, country_list, recycled_list  = list_initialization(excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "S49hjczAM1lP",
    "outputId": "06d54582-2173-4130-8362-93a1afaf8daa",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:25:12.203839Z",
     "end_time": "2023-08-11T13:25:12.226459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 random sentences added to queries and labels data.\n"
     ]
    }
   ],
   "source": [
    "queries, labels = new_sentences(1000, product_types, material_types, building_app_list, country_list, recycled_list, all_words='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3H462Oa3M1lP",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:50.748831Z",
     "end_time": "2023-08-11T13:24:50.870381Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Initialize_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "dKppEytPM1lU",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:50.818915Z",
     "end_time": "2023-08-11T13:24:50.916525Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = train_data_info(queries=queries, labels=labels, tokenizer=tokenizer)\n",
    "'''\n",
    "TOKEN CLASSIFICATION MODEL SAVE PATH LOCATION\n",
    "IF FIRST TIME TRAINING, OR NO FILE IS PASSED,\n",
    "IT WILL SAVE 'Token_Class_Model.pth' IN THIS FOLDER WHEN COMPLETE WITH TRAINING\n",
    "'''\n",
    "model_file = 'YOUR_TOKEN_CLASSIFICATION_MODEL.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "_brOqMeDM1lV",
    "outputId": "3b51d8e0-f389-49f3-ccc3-6a036a80d690",
    "ExecuteTime": {
     "start_time": "2023-08-11T13:24:50.839192Z",
     "end_time": "2023-08-11T13:24:50.947562Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_results = Token_Classification_Model_Training(Token_Classification_Labels=Token_Classification_Labels,\n",
    "                                                  Token_Classification_model=Token_Classification_model,\n",
    "                                                  train_dataloader=train_dataloader,\n",
    "                                                  test_dataloader=test_dataloader,\n",
    "                                                  Model_load_file=model_file,\n",
    "                                                  learning_rate=1e-5,\n",
    "                                                  epoch_loop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "djrkmvjIM1lV",
    "ExecuteTime": {
     "end_time": "2023-08-08T22:42:02.103829Z",
     "start_time": "2023-08-08T22:42:02.061818700Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results_dict \u001B[38;5;241m=\u001B[39m results_dict_creation(\u001B[43mraw_results\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'raw_results' is not defined"
     ]
    }
   ],
   "source": [
    "results_dict = results_dict_creation(raw_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IK_aybWeM1lV",
    "outputId": "855ca126-7e2f-464d-fe7b-4288710b7a70",
    "ExecuteTime": {
     "end_time": "2023-08-08T22:42:02.106830400Z",
     "start_time": "2023-08-08T22:42:02.075821100Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(results_dict)\n",
    "save_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sMjpATxM1lV",
    "outputId": "7b85de2b-87b9-4142-b137-b6f81ffdd026",
    "ExecuteTime": {
     "end_time": "2023-08-08T22:42:02.739472200Z",
     "start_time": "2023-08-08T22:42:02.092825Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for index in range(1, 5):\n",
    "    plot_creation(df.iloc[:, 0], df.iloc[:, index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSRJwLo-M1lV",
    "outputId": "6db73db6-f4d4-45db-b5e1-a7c49fac789b",
    "ExecuteTime": {
     "end_time": "2023-08-08T22:49:34.761745500Z",
     "start_time": "2023-08-08T22:49:34.541392500Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CREATE SENTENCE TO TEST MODEL\n",
    "'''\n",
    "list_of_sentences = ['TEST SENTENCE PASSED INTO MODEL TO DETERMINE OUTPUT']\n",
    "final_output = Token_Classification_Results(List_of_Sentences=list_of_sentences,\n",
    "                                            Token_Classification_Labels=Token_Classification_Labels,\n",
    "                                            Token_Classification_model=Token_Classification_model,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            Token_Class_File=model_file\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzJQPQErM1lV",
    "outputId": "672b2b50-82aa-42ec-a528-75ae02a6c2d5",
    "ExecuteTime": {
     "end_time": "2023-08-08T22:49:35.500878400Z",
     "start_time": "2023-08-08T22:49:35.478872900Z"
    }
   },
   "outputs": [],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T16:51:25.794118Z",
     "start_time": "2023-08-03T16:51:25.793613Z"
    },
    "id": "sMquOIAcM1lV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
